{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline\n",
    "\n",
    "the most basic object in transformers library\n",
    "\n",
    "it allows you to do a `task` using a `model`\n",
    "\n",
    "task: eg- `fill-mask`, `text-classification`, `text-generation`, `summarization`, `translation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_obj = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_obj(\n",
    "    \"I purchased a LinkedIn subscription and my expenses were over my budget\",\n",
    "    candidate_labels=[\"personal-finance\", \"money\", \"exercise\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pipeline(task=\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment([\"Attention mechanism is wild!\", \"What a disgrace to not understand the basics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt(\"In the cold winter of Toronto, the mailman was out and about without\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Good practice**: Choose model for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilgpt = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    # model=\"deepseek-ai/DeepSeek-R1\" # ImportError: cannot import name 'is_torch_greater_or_equal_than_1_13' from 'transformers.pytorch_utils' \n",
    "    # model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\" # model-00001-of-000004.safetensors = 8.71G\n",
    "    model=\"distilbert/distilgpt2\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilgpt(\n",
    "    text_inputs=\"In this huggingface NLP course we will\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "vocab = tiktoken.get_encoding(encoding_name=\"gpt2\")\n",
    "# same as the eos_token_id set in the above gpt-2 text generation\n",
    "vocab.n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer model families\n",
    "\n",
    "* **GPT like**: autoregressive transformer models\n",
    "* **BERT like**: autoencoding transformer models\n",
    "* **BART/T5 like**: sequence-to-sequence tranformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self-supervised learning:\n",
    "\n",
    "- type of learning used to develop transformers\n",
    "- objective is automatically computed from the input. (think the autoregressive target used in `Vaswani et.al. 2017`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transfer learning\n",
    "\n",
    "- in context of LLM's: self-supervised learning is not enough, so general pretrained models are **fine-tuned** using **transfer learning**\n",
    "\n",
    "ways of transfer learning:\n",
    "- causal language modeling\n",
    "  - task accomplished: next word prediction (given the previous n-words)\n",
    "  - GPT-2 was pretrained using this technique\n",
    "- masked language modeling\n",
    "  - task accomplished: predict a masked word in the sentence\n",
    "  - BERT was pretrained using this technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# carbon footprint\n",
    "\n",
    "LLM training equates to the total carbon emissions of 5 cars, throughout their lifetimes\n",
    "\n",
    "useful libraries: `codecarbon`\n",
    "\n",
    "online resources: [ML CO<sub>2</sub> Impact](https://mlco2.github.io/impact/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Why use pretrained model, instead of training model from scratch using the dataset specific to the task?\n",
    "\n",
    "**A.**\n",
    "\n",
    "- The pretrained model has statistical understanding of the language it was trained on.\n",
    "- The pretrained model was trained on much larger dataset than the fine-tuning dataset, so the fine-tuning dataset requires less training to get satisfactory results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Decoder\n",
    "\n",
    "- Encoder-decoder a.k.a sequence to sequence\n",
    "- **Encoder**: bidirectional, self-attention\n",
    "- **Decoder**: masked self-attention, autoregressive, unidirectional\n",
    "- These 2 can be used together or separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Encoder only model:** \n",
    "  - High dimensional representation of the inputs\n",
    "  - Good for tasks that require understanding the data: `text-classification (sentiment-analysis)`, `token-classification (ner)`\n",
    "- **Decoder only model:**\n",
    "  - Good for generative tasks: `text-generation`\n",
    "- **Encoder-Decoder model:**\n",
    "  - Generative tasks that require and input: `translation`, `summarization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
