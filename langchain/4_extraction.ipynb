{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction Chain\n",
    "\n",
    "topics covered:\n",
    "\n",
    "* tool calling\n",
    "* few shot prompting\n",
    "* chaining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field, EmailStr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema\n",
    "\n",
    "This is simply the schema for a **structured output**\n",
    "\n",
    "`description` -- this description is used by the LLM.\n",
    "\n",
    "Having a good description can help improve extraction results.\n",
    "\n",
    "## pydantic\n",
    "\n",
    "A robust way of defining data structures.\n",
    "\n",
    "Pydantic offers validation and json/dict serialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best practices for schema\n",
    "\n",
    "1. Document attributes and schema: The attribute `description` is directly used by the LLM, it can be helpful in quality improving of the outputs\n",
    "2. Prevent hallucination: Keep the outputs optional to avoid LLM from making up information incase it is absent. (Use `Optional` and `None`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "    name: Optional[str] = Field(default=None, description=\"Name of the person\")\n",
    "    hair_color: Optional[str] = Field(default=None, description=\"Color of the person's hair\")\n",
    "    height_in_meters: Optional[float] = Field(default=None, description=\"Height measured in meters\")\n",
    "    email: Optional[EmailStr] = Field(default=None, description=\"Email address of the person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractor\n",
    "\n",
    "It is a prompt template that contains appropriate context, instructions etc.\n",
    "\n",
    "Use it to optinally extract metadata about the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\",\n",
    "         \"You are an expert extraction algorithm. \"\n",
    "        \"Only extract relevant information from the text. \"\n",
    "        \"If you do not know the value of an attribute asked to extract, \"\n",
    "        \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        (\"user\",\n",
    "         \"{text}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    temperature=0,\n",
    "    model=\"llama3.2\"\n",
    ").with_structured_output(schema=Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am Ruchir Attri. You can recognize me from my black hair and 6 feet height. Feel free to reach me out on notmy@email.com\"\n",
    "\n",
    "prompt = prompt_template.invoke(\n",
    "    input={\"text\": text}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Ruchir Attri', hair_color='black', height_in_meters=1.83, email='notmy@email.com')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(\n",
    "    input=prompt\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple entities\n",
    "\n",
    "Use nested entities to find multiple occurences of a Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(BaseModel):\n",
    "    people: List[Person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    temperature=0,\n",
    "    model=\"llama3.2\"\n",
    ").with_structured_output(\n",
    "    schema=Data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[Person(name='Jeff', hair_color='black', height_in_meters=None, email=None), Person(name='Anna', hair_color='black', height_in_meters=None, email=None)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\n",
    "prompt = prompt_template.invoke({\"text\": text})\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot prompting\n",
    "\n",
    "It is a fancy way of saying: Give few example inputs to the LLM in your prompt\n",
    "\n",
    "Few shot prompting improves output quality\n",
    "\n",
    "**Q.** What does it look like?\n",
    "\n",
    "**A.** It is a sequence of pairs of: input(`user`) and expected response examples(`assistant`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    temperature=0,\n",
    "    model=\"llama3.2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='7', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-03-18T05:41:40.240551645Z', 'done': True, 'done_reason': 'stop', 'total_duration': 292844111, 'load_duration': 22285193, 'prompt_eval_count': 59, 'prompt_eval_duration': 46000000, 'eval_count': 2, 'eval_duration': 221000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-de14ef04-42be-4a7b-bfb7-a70f991673cb-0', usage_metadata={'input_tokens': 59, 'output_tokens': 2, 'total_tokens': 61})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"2 $ 2\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"4\"},\n",
    "    {\"role\": \"user\", \"content\": \"2 $ 3\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"5\"},\n",
    "    {\"role\": \"user\", \"content\": \"3 $ 4\"},\n",
    "]\n",
    "\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
