{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Useful in Tagging\n",
    "\n",
    "- think: tags on a library book (a document) on TPL website\n",
    "\n",
    "Label classes for a book could be:\n",
    "\n",
    "- language\n",
    "- topics covered\n",
    "- genre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook\n",
    "\n",
    "How to use tool calling function for tagging documents?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field # pydantic is used for validation (TODO: find out more on this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schema\n",
    "\n",
    "What properties do we want to extract from the prompt?\n",
    "\n",
    "`pydantic` simply allows us to define a schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Classification(BaseModel):\n",
    "#     sentiment: str = Field(description=\"The sentiment of the text in english\")\n",
    "#     melancholy: int = Field(description=\"How melancholic the text is on a scale of 1 to 10\")\n",
    "#     language: str = Field(description=\"The language the text is written in\")\n",
    "\n",
    "class Classification(BaseModel):\n",
    "    hello: str = Field(description=\"pick a letter between A and Z\")\n",
    "    # melancholy: int = Field(description=\"How melancholic the text is on a scale of 1 to 10\")\n",
    "    # language: str = Field(description=\"The language the text is written in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt template\n",
    "\n",
    "A `ChatPromptTemplate` to standardize the common bit (aka boilerplate) of prompts (here, the common bit is the instructions pertaining to a structured output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_template = ChatPromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "Extract desired information from the following passage.\n",
    "Only extract the information mentioned in the \"Classification\" class.\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM\n",
    "\n",
    "initialize the model to follow a structure for outputs (the schema defined above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    temperature=0,\n",
    "    model=\"llama3.2\"\n",
    ").with_structured_output(\n",
    "    schema = Classification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt\n",
    "\n",
    "combine the input and the prompt template to create a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"At vero eos et accusamus et iusto odio dignissimos ducimus, qui blanditiis praesentium voluptatum deleniti atque corrupti, quos dolores et quas molestias excepturi sint, obcaecati cupiditate non provident, similique sunt in culpa, qui officia deserunt mollitia animi, id est laborum et dolorum fuga.\"\n",
    "\n",
    "prompt = tagging_template.invoke(\n",
    "    input={\"input\": inp}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "Finally, pass the prompt to the LLM\n",
    "\n",
    "Remember what effort has gone into preparing the prompt (literally prompt engineering):\n",
    "\n",
    "1. The prompt template giving a common context to each input\n",
    "2. The schema for structured output clearly defining what output is expected\n",
    "3. The LLM initialized with schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classification(hello='s')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(\n",
    "    input=prompt\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why you do this?\n",
    "\n",
    "Think of **structured output** as an extremely particular way of getting the LLM to answer back.\n",
    "\n",
    "We're used to seeing Chat GPT blurt out paragraphs on paragraphs.\n",
    "\n",
    "We can make better use of the output if we can pass it around an application, which is done with API, which have schema or a structure in general.\n",
    "\n",
    "Hence we need structured output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice\n",
    "\n",
    "OpenAI uses a moderation API to filter potentially harmful input.\n",
    "\n",
    "I thought of recreating this functionality using structured outputs\n",
    "\n",
    "its as simple as defining the harmful categories in the schema.\n",
    "\n",
    "This implementation is very rudimentary and not practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counter Strike 2 chat moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Moderation(BaseModel):\n",
    "    violence: float = Field(description=\"Rate the violence in the sentence between 0 to 1\")\n",
    "    hate: float = Field(description=\"Rate the hate in the sentence between 0 to 1\")\n",
    "    hacking: float = Field(description=\"Rate the extent of hacking in the sentence between 0 to 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderation_model = ChatOllama(\n",
    "    temperature=0,\n",
    "    model=\"llama3.2\"\n",
    ").with_structured_output(\n",
    "    schema=Moderation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderation_template = ChatPromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "Give the scores for the potentially harmful categories defined in \"Moderation\" class.\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"I swear I will shoot down all puny terrorists who try to peek at A long site!\"\n",
    "\n",
    "prompt = moderation_template.invoke(\n",
    "    input={\"input\": inp}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Moderation(violence=0.8, hate=0.9, hacking=0.0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = moderation_model.invoke(\n",
    "    input=prompt\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Moderation(violence=0.0, hate=0.0, hacking=0.0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"Shut up SkreemingKroos, you're trash. you have 1-5 KD and chatting shit!?\"\n",
    "\n",
    "prompt = moderation_template.invoke(\n",
    "    input=inp\n",
    ")\n",
    "\n",
    "response = moderation_model.invoke(\n",
    "    input=prompt\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Moderation(violence=0.0, hate=1.0, hacking=1.0)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"Yo CT, vote out fortnitelover2 he's got aimbot and wall X-ray\"\n",
    "\n",
    "prompt = moderation_template.invoke(\n",
    "    input=inp\n",
    ")\n",
    "\n",
    "response = moderation_model.invoke(\n",
    "    input=prompt\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note**: why is my output score not between 0 and 1 when I have mentioned it in the schema??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Moderation(violence=0.0, hate=0.0, hacking=1.0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"bruh fiddlejiddle12 got suspicious peek at middle\"\n",
    "\n",
    "prompt = moderation_template.invoke(\n",
    "    input=inp\n",
    ")\n",
    "\n",
    "response = moderation_model.invoke(\n",
    "    input=prompt\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
